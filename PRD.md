FeedbackAgent – Combined MVP PRD (24h Hackathon)

Status: Draft  
 Timeline: 24h (2 devs, \~12h coding)  
 Priority: Prove the loop, not productionize it

---

## Soulcaster Snapshot (Plan vs Reality)

This is a quick, top‑level comparison; the rest of the document is the original PRD.

| Area            | Originally Planned                                         | Implemented in Soulcaster                                      | Extra / Out of Scope                          |
|-----------------|-----------------------------------------------------------|-----------------------------------------------------------------|-----------------------------------------------|
| Ingestion       | Reddit (PRAW), Sentry webhooks, normalize to FeedbackItem | Reddit JSON poller, Sentry, manual text; dedupe via external_id | GitHub issue ingestion via dashboard routes   |
| Storage         | In‑memory MVP, later Redis/Postgres                       | In‑memory + Redis/Upstash with store abstraction               | Rich Redis schema for jobs + clustering       |
| Clustering      | Backend embeddings, centroid‑based clustering per ingest  | Simple backend buckets; vector clustering via dashboard + Vector | Cohesion metrics and re-cluster APIs          |
| Dashboard       | Clusters list + detail, “Generate Fix”                    | Full Next.js App Router UI, stats, config, trigger‑agent flows | GitHub OAuth, SourceConfig UI                 |
| Coding Agent    | Backend‑embedded PyGithub agent, Python syntax check      | Separate `coding-agent` service, ECS Fargate runner, job tracking | Kilo/Gemini integration, issue-first flow     |
| DB Hardening    | Postgres schema + indices (post‑hackathon)                | Not implemented yet (Redis only)                               | —                                             |
| Observability   | Minimal logging, no robustness                            | Minimal logging and tests (more than planned, still non-prod)   | Expanded tests across backend/dashboard/agent |

For a narrative implementation report, see section **10. Implementation Report (Soulcaster vs This PRD)** at the end.

---

## **1\. Executive Summary**

Build a “self-healing dev loop” MVP:

Reddit/Sentry → Clustered Issues → human reviews on triage dashboard \-\> One‑Click Fix → GitHub PR

Constraints:

* Single target repo.

* Happy-path only.

* Prioritize functional code over robustness, tests, or scale.  
* Test Driven Development (TDD)

---

## **2\. Scope & Guardrails**

### **2.1 What We’re Building (MVP)**

* Python ingestion service:

  * Polls Reddit for bug reports (PRAW).

  * Receives Sentry webhooks.

* LLM-based triage:

  * Embedding-based clustering of feedback into “issues.”

  * LLM summaries per cluster.

* Next.js dashboard on Vercel:

  * Shows clustered issues.

  * One‑click “Generate Fix.”

* Coding agent (Python):

  * Uses GitHub API (PyGithub) to fetch candidate files.

  * Generates code patch via LLM.

  * Runs basic code reviews, optionally with CodeRabbit.

  * Opens a real PR on GitHub.

### **2.2 What We’re NOT Building (for 24h)**

* No auth/permissions (hardcoded keys / env only).

* No multi-repo routing (one repo only).

* No historical backfill or complex search.

* No robust retries / rate limiting / observability.

* No chat UI (this is “click to fix,” not a chatbot).

* No serious test suite (manual happy-path verification only).

---

## **3\. System Overview – The Loop**

* **The Ears (Ingestion)**  
   Reddit poller \+ Sentry webhook → normalized feedback items.

* **The Brain (Triage \+ Coding)**  
   Embedding-based clustering \+ LLM summaries → coding agent that turns a cluster into a patch.

* **The Hands (Execution)**  
   GitHub API (and optionally Actions later) → branch \+ commit \+ PR.

---

## **4\. MVP Scope Details**

### **4.1 Ingestion Layer (The Ears)**

#### **Reddit Poller**

* Script: `reddit_poller.py` (Python).

* Tech: PRAW.

* Behavior:

  * Poll specified subreddits every N minutes (e.g., 1–5).

  * Filter posts/comments by keywords: `"bug"`, `"broken"`, `"error"`, `"crash"`, `"doesn't work"`, `"feature"`.

  * Normalize to `FeedbackItem` and send to backend via `POST /ingest/reddit`.

* Normalized `FeedbackItem` fields (for Reddit):

  * `id` (internal UUID).

  * `source = "reddit"`.

  * `external_id` (Reddit ID).

  * `title` (post title or first 80 chars of comment).

  * `body` (full text, truncated to max length).

  * `metadata` (subreddit, permalink, author, created\_utc).

  * `created_at`.

#### **Sentry Webhook**

* Endpoint: `POST /ingest/sentry` (FastAPI).

* Body: Sentry event JSON.

* Behavior:

  * Extract:

    * `title` (Sentry issue title).

    * `body` \= error message \+ top 1–3 stack frames (filename:line).

    * `metadata` \= project, environment, release, tags, event\_id, timestamp.

  * Normalize to `FeedbackItem` with:

    * `source = "sentry"`, `external_id = event_id`.

Storage (MVP):

* Keep `FeedbackItem`s in an in-memory store (Python dict or list).

* Post‑Hackathon: swap to Redis/Supabase.

---

### **4.2 Triage Agent (The Brain – Part 1\)**

Goal: Turn noisy feedback into deduped “Issue Clusters” for humans to act on.

#### **Data Model (Conceptual)**

* `FeedbackItem`:

  * `id`

  * `source` ("reddit" | "sentry")

  * `external_id`

  * `title`

  * `body`

  * `metadata` (JSON)

  * `created_at`

  * `embedding` (vector or opaque blob)

* `IssueCluster`:

  * `id`

  * `title` (LLM-generated issue title)

  * `summary` (2–3 sentence description)

  * `feedback_ids` (list of `FeedbackItem.id`)

  * `status` ("new" | "fixing" | "pr\_opened" | "failed")

  * `created_at`

  * `updated_at`

  * `embedding_centroid` (vector)

  * `github_branch` (optional)

  * `github_pr_url` (optional)

  * `error_message` (optional for failures)

#### **Clustering Logic (Happy-Path)**

On ingest of a new `FeedbackItem`:

1. Compute embedding for `body` (or `title + body`).

   * Threshold-based nearest neighbor.

   * Similarity metric: cosine.

2. Compare to all open clusters’ centroids:

   * If best similarity ≥ 0.8–0.85:

     * Attach feedback to that cluster:

       * Append `feedback_id`.

       * Update centroid as mean of member embeddings.

   * Else:

     * Create new `IssueCluster` with this single feedback.

3. Summarization:

   * For any new cluster:

     * Call LLM with up to N sample feedback bodies (e.g., up to 5).

     * Output JSON:

       * `title`: ≤80 characters.

       * `summary`: ≤300 characters.

No manual merge/split in MVP.

#### **Triage Endpoints (Backend)**

* `GET /clusters`

  * Returns list: `id`, `title`, `summary`, `count`, `status`, `sources[]`, `github_pr_url?`.

* `GET /clusters/{id}`

  * Returns cluster \+ sample `FeedbackItem`s.

* `POST /clusters/{id}/start_fix`

  * Sets `status="fixing"` and triggers Coding Agent.

---

### **4.3 Coding Agent (The Brain – Part 2\)**

Goal: For a chosen cluster, generate a minimal code patch and open a PR.

Assumptions:

* One target GitHub repo.

* MVP target language: Python (for syntax check).

* No tests run, just syntax sanity.

#### **Context Construction**

Inputs:

* Cluster data:

  * `title`, `summary`, up to 3 raw feedback texts.

* Repo data:

  * File path list (from GitHub API, cached).

  * Optional: repo description/stack hints from env.

#### **Candidate File Selection**

Heuristics:

1. From Sentry-based feedback:

   * Extract filenames from stack frames (regex on `filename` fields).

   * Match against repo paths (suffix match or contains).

2. From text:

   * Extract keywords from cluster title.

   * Select files whose path contains those tokens (case-insensitive).

3. Limit to:

   * Max 3–5 candidate files.

   * Max size per file (e.g., 10k chars).

If nothing found:

* Fallback: send a truncated file path list to the LLM and ask it to select 1–2 paths.

#### **Patch Generation (LLM)**

Prompt:

* Explain bug (cluster summary \+ feedback snippets).

* Include file path \+ current content for each candidate.

* Instruct LLM:

  * Make the smallest possible change that plausibly fixes this issue.

  * Do not refactor unrelated code.

Return JSON:

 {  
  "files": \[  
    {  
      "path": "path/to/file.py",  
      "updated\_content": "\<full file content after edit\>"  
    }  
  \],  
  "summary": "Short human-readable description of the change."  
}

* 

MVP: we do full-file replacement, not diff parsing.

#### **Syntax Check**

* For each modified `.py` file:

  * Run `ast.parse(updated_content)` in Python.

* If any parse fails:

  * Mark cluster `status="failed"`.

  * Store parse error in `error_message`.

* If parse passes (or file is non-Python), proceed.

(Real multi-language linting is Post‑Hackathon.)

#### **Branch & PR Creation (Execution Layer – The Hands)**

Using PyGithub:

1. Resolve target repo & base branch (`GITHUB_REPO`, `BASE_BRANCH`).

2. Create branch name:

   * `feedbackagent/cluster-{id}-{slugified-title}`.

3. For each modified file:

   * Fetch current file via `get_contents(path, ref=base_branch)` to get `sha`.

   * Use `update_file` / `create_file` on new branch with `updated_content`.

4. Create PR:

   * Title: `Fix: {cluster.title}`

   * Body:

     * LLM `summary`.

     * Cluster `summary`.

     * List of feedback sources (links to Reddit/Sentry where available).

     * Note: “Auto-generated by FeedbackAgent; please review carefully.”

5. Update `IssueCluster`:

   * `status="pr_opened"`

   * `github_branch`, `github_pr_url`.

Failure: any exception marks `status="failed"` and logs `error_message`. No automatic retries in MVP.

---

### **4.4 Dashboard (Next.js on Vercel)**

Minimal but usable.

#### **Views**

**Clusters List (Main Page)**

* Table of `IssueCluster`s:

  * Columns:

    * Title

    * Summary (truncated)

    * Count (\# feedback)

    * Sources (icons: Reddit / Sentry)

    * Status badge:

      * `new`, `fixing`, `pr_opened`, `failed`

    * Actions:

      * “Generate Fix” button (enabled if `status` in \[`new`, `failed`\]).

**Cluster Detail View**

* Route: `/clusters/[id]` or modal.

* Shows:

  * Title, summary, created/updated timestamps.

  * Counts by source (e.g., 3 Reddit, 2 Sentry).

  * Sample feedback list:

    * Text snippet.

    * Link to Reddit or Sentry issue page when available.

  * If `github_pr_url`:

    * Button: “View PR in GitHub.”

  * If `status="failed"`:

    * Show `error_message`.

#### **Dashboard–Backend API Contract**

Next.js API routes proxy to backend:

* `GET /api/clusters` → backend `/clusters`

* `GET /api/clusters/[id]` → backend `/clusters/{id}`

* `POST /api/clusters/[id]/start_fix` → backend `/clusters/{id}/start_fix`

Behavior:

* When “Generate Fix” clicked:

  * Call `start_fix`.

  * Optimistically set state to “Running…” and poll detail endpoint until `status` is `pr_opened` or `failed`.

---

## **5\. Technical Architecture**

### **5.1 Stack**

* Frontend:

  * Next.js (App Router).

  * Hosted on Vercel.

  * UI library: optional (e.g., shadcn/ui), but plain Tailwind is fine.

* Backend:

  * Python \+ FastAPI (recommended).

  * Single service:

    * Ingestion endpoints.

    * Triage logic.

    * Coding agent.

    * GitHub integration.

* Storage (MVP):

  * In-memory Python dicts for:

    * `feedback_items`

    * `issue_clusters`

  * No durable DB required to prove the loop.

* LLM / Embeddings:

  * Model-agnostic via wrapper (e.g., LiteLLM).

  * Recommended split:

    * Fast/cheap model (e.g., Gemini Flash-style) for embeddings & summaries.

    * Stronger model (e.g., Claude Sonnet-style) for code generation.

* Queue:

  * Simple in-process background tasks (threads / asyncio).

  * No external queue system.

### **5.2 Key Libraries / Tools**

* PRAW – Reddit polling.

* PyGithub – GitHub repo access \+ PR creation.

* FastAPI – Backend HTTP server.

* Next.js – Dashboard.

* LiteLLM (optional) – Model routing / abstraction.

### **5.3 TODO Decisions for Humans**

* LLM provider \+ models for:

  * summarization,

  * embeddings,

  * code generation.

* Storage choice:

  * stick with in-memory for demo vs. wire Supabase/Redis.

* Deployment target for Python backend (Railway/Render/fly/etc.).

* Single target repo \+ branch conventions.

---

## **6\. User Flow (Happy Path)**

1. **User reports bug**

   * Redditor posts “Export crashes on Safari” in your project subreddit.

   * Or a Sentry event fires: “Unhandled exception in export handler.”

2. **FeedbackAgent ingests**

   * Reddit poller detects keywords and posts to `/ingest/reddit`.

   * Sentry sends webhook to `/ingest/sentry`.

   * Backend creates `FeedbackItem` records.

3. **Triage agent clusters**

   * New feedback gets an embedding.

   * Similarity ≥ threshold with existing “Export crash” cluster → added there.

   * Cluster summary is updated or created via LLM.

4. **Dashboard shows cluster**

   * Next.js dashboard calls `/clusters`.

   * “Export crash in Safari” appears as a row with 4 reports (3 Reddit, 1 Sentry).

5. **Human clicks “Generate Fix”**

   * Dev opens cluster detail, confirms it’s legit.

   * Clicks “Generate Fix.”

6. **Coding agent generates patch**

   * Backend:

     * Marks status `fixing`.

     * Picks candidate files (e.g., `components/ExportButton.tsx` or `export.py`).

     * Calls LLM with context to produce updated file content.

     * Runs basic syntax check (Python).

7. **PR is opened**

   * Backend:

     * Creates branch \+ commit.

     * Opens PR: `Fix: Export crash in Safari`.

   * Cluster status set to `pr_opened`.

   * Dashboard shows PR link.

8. **Human/bot reviews & merges**

   * Human or CodeRabbit reviews PR.

   * After merge, FeedbackAgent is done; status can be updated manually post-MVP if desired.

---

## **7\. Implementation Plan (2 devs, \~12h)**

Assume:

* Dev A – Python/backend/agents.

* Dev B – Next.js/frontend.

Time is approximate; keep scope ruthless.

### **Phase 1 – Foundations (“The Pipes”) – \~3h**

**Dev A**

* Scaffold FastAPI app with endpoints:

  * `POST /ingest/reddit`

  * `POST /ingest/sentry`

  * `GET /clusters`

  * `GET /clusters/{id}`

  * `POST /clusters/{id}/start_fix` (stub).

* Implement in-memory stores:

  * `feedback_items` map.

  * `issue_clusters` map.

* Implement very naive clustering (no embeddings yet, just new cluster per feedback) to unblock UI.

**Dev B**

* Scaffold Next.js app on Vercel (or locally).

* Create `/` page with static mock cluster list.

* Create basic table UI with:

  * Title, count, status, “Generate Fix” (dummy click).

### **Phase 2 – Triage (“The Brain – Part 1”) – \~3h**

**Dev A**

* Implement `EmbeddingClient` and `LLMClient` wrappers (using chosen provider).

* Implement real clustering:

  * Embedding per feedback.

  * Similarity threshold logic.

  * Centroid maintenance.

* Implement LLM summarization for new clusters.

**Dev B**

* Wire `/` page to `GET /api/clusters` → backend.

* Implement cluster detail page:

  * `GET /api/clusters/[id]`.

  * Show sample feedback items.

Milestone: Real clusters and summaries appear on dashboard.

### **Phase 3 – Coding Agent (“The Brain – Part 2”) – \~3h**

**Dev A**

* Integrate PyGithub:

  * Connect to target repo.

  * Cache file tree.

* Implement candidate file selection heuristics (stack traces \+ title keywords).

* Implement patch generation:

  * Prompt building.

  * LLM call.

  * JSON parsing into `{path, updated_content}` objects.

* Implement syntax check (Python `ast.parse`).

**Dev B**

* Implement `POST /api/clusters/[id]/start_fix` → backend.

* Add UI state for “fixing”:

  * Disable button.

  * Poll cluster status on interval.

Milestone: “Generate Fix” triggers code generation, syntax check, but PR may still be stubbed.

### **Phase 4 – PR Integration (“The Hands”) – \~2h**

**Dev A**

* Implement branch \+ commit \+ PR creation flow via PyGithub.

* Set cluster `status` and `github_pr_url`.

* Ensure basic error handling for obvious failures.

**Dev B**

* Show PR link on cluster detail page when `github_pr_url` present.

* Show error message for `status="failed"`.

Milestone: End-to-end: ingest → cluster → click → PR.

### **Phase 5 – Polish & Demo – \~1h**

**Both**

* Add minimal logging.

* Seed example feedback (curl scripts, JSON fixtures).

* Update README:

  * Setup env vars.

  * How to run backend/frontend.

  * How to configure Reddit/Sentry/GitHub.

* Run final demo: record or rehearse.

---

## **8\. Future Scope (Tagged)**

These are explicitly Post‑Hackathon. Tags in brackets.

* Replace in-memory storage with real DB

  * `[db change]` Supabase Postgres schema for `feedback_items`, `issue_clusters`, embeddings.

  * `[db change]` Add indices on `status`, `created_at` for faster dashboard queries.

* Improve infra and frameworks

  * `[framework changes]` Introduce Redis for queues and caching embeddings/file trees.

  * `[framework changes]` Add GitHub Actions pipeline to: checkout repo, apply patch, run tests, then open PR.

* Enhance UI/UX

  * `[ui changes]` Cluster merge/split controls, labels, and severity tags.

  * `[ui changes]` Filters (source, status, timeframe), search, and pagination.

  * `[ui changes]` Timeline view of incidents per cluster.

* Smarter agent behavior

  * `[agent logic]` Multi-language syntax checks (TS/JS/Go/… via appropriate tools).

  * `[agent logic]` Limited RAG over codebase (index core modules, not full monorepo).

  * `[agent logic]` Auto-close clusters when linked PR merges.

* Productization

  * `[auth/security]` GitHub OAuth and per-user repos/projects.

  * `[auth/security]` Signed webhooks and strict token scoping.

  * `[ops]` Metrics, tracing, structured logs, rate limiting, and retry policies.

---

## **9\. Success Metrics (for MVP)**
	
* ≥1 real PR opened from real Reddit or Sentry feedback.
	
* Dashboard shows clustered issues with sensible summaries.
	
* Full loop (ingest → cluster → click → PR) completes in \<2 minutes on average for small repos.
	
* Devs feel comfortable reviewing, not re-writing, the auto-generated PR.
	
---

## **10\. Implementation Report (Soulcaster vs This PRD)**

This section gives a high‑level report of how the current Soulcaster codebase lines up with the original hackathon PRD: what was planned and built, what’s still open, and what we added that wasn’t originally in scope. The detailed PRD text above is left unchanged; this is a summary overlay.

### **10.1 Planned & Implemented**

These items were explicitly in the PRD and exist in the current codebase:

* Ingestion:
  * Reddit ingest endpoint `POST /ingest/reddit` and a poller script (`backend/reddit_poller.py`), plus Sentry ingest via `POST /ingest/sentry`.
  * Manual text ingestion `POST /ingest/manual`, which matches the “manual feedback” idea even though it was not fully spelled out in the original doc.
  * Deduplication on `(source, external_id)` via `get_feedback_by_external_id` and `feedback:external:{source}:{external_id}` in `backend/store.py`.
* Storage:
  * In‑memory storage and Redis/Upstash storage behind a store abstraction (`backend/store.py`), matching the “in‑memory first, easy to swap” constraint.
  * Redis keys for feedback and clusters (`feedback:{id}`, `feedback:created`, `feedback:source:{source}`, `cluster:{id}`, `cluster:items:{id}`), as outlined in the original DB design doc.
* Clusters & dashboard:
  * Backend cluster APIs `GET /clusters`, `GET /clusters/{id}`, `POST /clusters/{id}/start_fix`.
  * A Next.js dashboard (App Router) showing clusters, feedback counts, statuses, and a “Generate Fix” action wired through `/api/clusters/[id]/start_fix`.
* Coding agent & PR loop:
  * A coding agent that opens real GitHub PRs, using repository context plus issue/cluster data.
  * Basic job tracking for “Generate Fix” via `/jobs` endpoints on the backend (status, logs) consumed by the dashboard.

### **10.2 Planned but Not Yet Implemented or Only Partially Implemented**

These items are called out in the PRD but are not fully wired, or exist only in a simplified form:

* Clustering and summaries:
  * Always‑on embedding‑based clustering in the backend on each ingest (with centroids and cosine thresholds) is not implemented; backend clustering is currently simple bucketing by source and subreddit/title.
  * LLM‑generated cluster titles and summaries exist in the dashboard vector clustering flow, but the backend’s `_auto_cluster_feedback` still uses static titles/summaries.
  * Cluster merge/split operations, severity tags, advanced filters/search, and timelines are still future work.
* Coding agent behavior:
  * The PRD’s backend‑embedded coding agent with PyGithub and Python‑only syntax checks is conceptually there, but the actual implementation is a separate `coding-agent` service rather than living inside the FastAPI app.
  * Multi‑language syntax checks, stronger RAG over the codebase, and auto‑closing clusters on PR merge remain in the “future scope” category.
* Storage & infra:
  * The “swap to Postgres/Supabase” part of the plan (with explicit SQL schema) has not been built; Redis/Upstash plus in‑memory fallback are the only stores today.
  * Production‑grade observability (metrics, tracing, rate limits, retries) is still minimal, consistent with the hackathon constraint but short of a hardened system.
* Productization:
  * Full auth and multi‑tenant/multi‑repo routing are not implemented end‑to‑end; GitHub OAuth exists on the dashboard via NextAuth, but backend APIs are still keyed by env/config.

### **10.3 Not in Original Scope but Implemented (Extras)**

These pieces were not captured in the original 24h PRD but exist in the Soulcaster codebase today:

* Vector‑based clustering in the dashboard:
  * Upstash Vector + Gemini‑based clustering pipeline implemented in `dashboard/lib/vector.ts` and `/app/api/clusters/run-vector`, operating on `feedback:unclustered`, `cluster:{id}`, `cluster:items:{id}`, and `clusters:all`.
  * Cohesion scoring and similarity‑based grouping beyond the simple centroid model envisioned in this document.
* Separate coding agent service + AWS runner:
  * A standalone `coding-agent/fix_issue.py` CLI, wrapped in a container and launched as an ECS Fargate task via `/app/api/trigger-agent`.
  * Backend job tracking (`job:{id}`, `cluster:jobs:{cluster_id}`) connected to the agent via `BACKEND_URL`/`JOB_ID`, with logs and status updates surfaced in the dashboard.
* Extra ingestion and config flows:
  * GitHub issue ingestion and sync routes on the dashboard, writing normalized feedback directly into Redis, beyond just Reddit/Sentry.
  * A Reddit subreddit config UI backed by `config:reddit:subreddits` shared between dashboard and backend.
* Tests and documentation:
  * A broader automated test surface for backend, dashboard, and coding agent than the original PRD’s “no serious test suite” constraint.
  * Additional architecture docs: `documentation/current_architecture.md` and the enriched Redis plan in `documentation/db_design.md`.

For a deeper system‑level view, see `documentation/current_architecture.md`, and for the exact Redis keyshapes backing this loop, see `documentation/db_design.md`.
